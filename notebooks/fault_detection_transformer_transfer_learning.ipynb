{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T12:34:55.672351Z",
     "start_time": "2025-06-26T12:34:49.415487Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "window_size = 16\n",
    "batch_size = 64\n",
    "input_dim = 30\n",
    "num_heads = 2\n",
    "d_model = 64\n",
    "num_encoders = 1\n",
    "num_decoders = 1\n",
    "lr=0.001\n",
    "\n",
    "exp_name = f\"random_{window_size}_{batch_size}_{input_dim}_{num_heads}_{num_encoders}_{num_decoders}_{d_model}\"\n",
    "a1_attack_values_normalized_df = pd.read_csv(\"../datasets/a1_attack_values_normalized_df.csv\")\n",
    "a6_attack_values_normalized_df = pd.read_csv(\"../datasets/a6_attack_values_normalized_df.csv\")\n",
    "checkpoint_dir = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T12:34:55.686801Z",
     "start_time": "2025-06-26T12:34:55.673683Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Transformer-based Feature Extractor\n",
    "class TransformerFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=4, num_enc_layers=2, num_dec_layers=2, d_model=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = nn.Parameter(self.create_sinusoidal_encoding(16, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, batch_first=True)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=num_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_enc_layers)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_dec_layers)\n",
    "\n",
    "    def create_sinusoidal_encoding(self, seq_length, hidden_dim):\n",
    "        position = torch.arange(seq_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2) * -(math.log(10000.0) / hidden_dim))\n",
    "        pe = torch.zeros(seq_length, hidden_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # Shape: (1, seq_length, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded, encoded)\n",
    "        return decoded[:, -1, :]\n",
    "\n",
    "# Define the Classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Full Model Combining Feature Extractor & Classifier\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=4, num_enc_layers=2, num_dec_layers=2, num_classes=2, d_model=128):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = TransformerFeatureExtractor(input_dim, num_heads, num_enc_layers, num_dec_layers, d_model)\n",
    "        self.classifier = Classifier(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        return self.classifier(features)\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, window_size, rolling_window = 1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): The input DataFrame.\n",
    "            window_size (int): The size of the window.\n",
    "            sensor_columns (list): List of column names for sensor readings.\n",
    "            target_column (str): Name of the target column.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.rolling_window = rolling_window\n",
    "        self.sensor_columns = [\"FIT101\", \"LIT101\", \"MV101\", \"P101\", \"AIT201\", \"AIT202\", \"AIT203\", \"FIT201\", \"MV201\", \"P203\", \"DPIT301\", \"FIT301\", \"LIT301\", \"MV301\", \"MV302\", \"MV303\", \"MV304\", \"AIT402\", \"FIT401\", \"LIT401\", \"AIT501\", \"AIT502\", \"AIT503\", \"AIT504\", \"FIT501\", \"FIT502\", \"FIT503\", \"PIT501\", \"PIT502\", \"PIT503\"]\n",
    "        self.target_column = \"Target\"\n",
    "        self.data, self.labels = self._create_windows()\n",
    "\n",
    "    def _create_windows(self):\n",
    "        \"\"\"\n",
    "        Create windows of size `window_size` from the DataFrame.\n",
    "        Each window contains consecutive samples of the same class.\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        labels = []\n",
    "        class_groups = self.df.groupby(self.target_column)\n",
    "\n",
    "        for _, group in class_groups:\n",
    "            group = group.sort_index()  # Ensure data is in order\n",
    "            group_values = group[self.sensor_columns].values\n",
    "            group_length = len(group)\n",
    "            if self.rolling_window == 0:\n",
    "                for i in range(np.int16(group_length / self.window_size)):\n",
    "                    window = group_values[i * self.window_size:i * self.window_size + self.window_size]\n",
    "                    data.append(window)\n",
    "                    labels.append(group[self.target_column].iloc[i])  # Use the class of the first sample in the window\n",
    "\n",
    "            else:\n",
    "                # Create windows for this group\n",
    "                for i in range(group_length - self.window_size + 1):\n",
    "                    window = group_values[i:i + self.window_size]\n",
    "                    data.append(window)\n",
    "                    labels.append(group[self.target_column].iloc[i])  # Use the class of the first sample in the window\n",
    "\n",
    "        return np.array(data), np.array(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a window of data and its corresponding label.\n",
    "        \"\"\"\n",
    "        window = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(window, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T12:34:56.350689Z",
     "start_time": "2025-06-26T12:34:55.687695Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_time_series_data(df, window_size=16):\n",
    "    # Splitting dataframe based on 'Target' class\n",
    "    class_0 = df[df[\"Target\"] == 0]\n",
    "    class_1 = df[df[\"Target\"] == 1]\n",
    "\n",
    "    def create_windows(df, window_size):\n",
    "        \"\"\"Creates non-overlapping windows from a dataframe.\"\"\"\n",
    "        num_windows = len(df) // window_size\n",
    "        windows = [df.iloc[i * window_size: (i + 1) * window_size] for i in range(num_windows)]\n",
    "        return windows\n",
    "\n",
    "    class_0_windows = create_windows(class_0, window_size)\n",
    "    class_1_windows = create_windows(class_1, window_size)\n",
    "\n",
    "\n",
    "    def split_and_shuffle(windows):\n",
    "        \"\"\"Splits and shuffles the windows into train, validation, and test sets.\"\"\"\n",
    "        train, val = train_test_split(windows, test_size=0.15, random_state=42)\n",
    "        train, test = train_test_split(train, test_size=0.13, random_state=42)\n",
    "\n",
    "        return train, val, test\n",
    "\n",
    "    train_0, val_0, test_0 = split_and_shuffle(class_0_windows)\n",
    "    train_1, val_1, test_1 = split_and_shuffle(class_1_windows)\n",
    "\n",
    "    # Combine the train, val, and test sets for both classes\n",
    "    train_windows = train_0 + train_1\n",
    "    val_windows = val_0 + val_1\n",
    "    test_windows = test_0 + test_1\n",
    "\n",
    "    # Shuffle the combined sets\n",
    "    np.random.shuffle(train_windows)\n",
    "    np.random.shuffle(val_windows)\n",
    "    np.random.shuffle(test_windows)\n",
    "\n",
    "    # Convert windows back to DataFrame and sort by index\n",
    "    def reconstruct_sorted_df(windows):\n",
    "        df = pd.concat(windows).sort_index()\n",
    "        return df\n",
    "\n",
    "    train_df = reconstruct_sorted_df(train_windows)\n",
    "    val_df = reconstruct_sorted_df(val_windows)\n",
    "    test_df = reconstruct_sorted_df(test_windows)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    source_df = a1_attack_values_normalized_df.copy()\n",
    "    target_df = a6_attack_values_normalized_df.copy()\n",
    "\n",
    "    train_df, val_df, test_df = prepare_time_series_data(source_df, window_size=window_size)\n",
    "    target_train_df, target_val_df, target_test_df = prepare_time_series_data(target_df, window_size=window_size)\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = TimeSeriesDataset(train_df, window_size), TimeSeriesDataset(val_df, window_size), TimeSeriesDataset(test_df, window_size)\n",
    "    target_train_dataset, target_val_dataset, target_test_dataset = TimeSeriesDataset(target_train_df, window_size), TimeSeriesDataset(target_val_df, window_size), TimeSeriesDataset(target_test_df, window_size, 0)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, target_train_dataset, target_val_dataset, target_test_dataset\n",
    "\n",
    "def count_each_class_instances(dataloader_object):\n",
    "    class_count = {}\n",
    "    for _, labels in dataloader_object:\n",
    "        labels = np.array(labels)\n",
    "        for label in labels:\n",
    "            if label in class_count.keys():\n",
    "                class_count[label] += 1\n",
    "            else:\n",
    "                class_count[label] = 0;\n",
    "    return class_count\n",
    "\n",
    "\n",
    "# Data Loaders\n",
    "train_dataset, val_dataset, test_dataset, target_train_dataset, target_val_dataset, target_test_dataset = load_data()\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "target_train_loader = DataLoader(target_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "target_val_loader = DataLoader(target_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "target_test_loader = DataLoader(target_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_count = count_each_class_instances(train_loader)\n",
    "val_count = count_each_class_instances(val_loader)\n",
    "test_count = count_each_class_instances(test_loader)\n",
    "\n",
    "target_train_count = count_each_class_instances(target_train_loader)\n",
    "target_val_count = count_each_class_instances(target_val_loader)\n",
    "target_test_count = count_each_class_instances(target_test_loader)\n",
    "\n",
    "print(f\"Train Dataset Count: {train_count}, Validation Data count: {val_count}, Test Count: {test_count}\")\n",
    "print(f\"Target Train: {target_train_count}, Target Validation: {target_val_count}, Target test Count: {target_test_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for previous model checkpoints and resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T12:34:56.873851Z",
     "start_time": "2025-06-26T12:34:56.352186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Feature Extractor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "model = TransformerClassifier(input_dim, num_heads, num_encoders, num_decoders, num_classes, d_model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loaded_epoch = 0\n",
    "best_accuracy = 0\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists(os.path.join(checkpoint_dir, exp_name, \"source\")):\n",
    "    check_dir = os.path.join(checkpoint_dir, exp_name, \"source\")\n",
    "    os.makedirs(check_dir)\n",
    "    os.makedirs(os.path.join(checkpoint_dir, exp_name, \"target\"))\n",
    "    print(f\"no checkpoint for model: {exp_name}, make a new one at {check_dir}\")\n",
    "    best_step = 0\n",
    "else:\n",
    "    if not os.path.exists(os.path.join(checkpoint_dir, exp_name,\"source\",'model_best.pth.tar')):\n",
    "        best_step = 0\n",
    "        print(os.path.join(checkpoint_dir, exp_name,\"source\",'model_best.pth.tar'))\n",
    "        print(\"Couldn't find the model, creating new one.\")\n",
    "    else:\n",
    "        print(f\"Found a checkpoint, loading checkpoint from {check_dir}\")\n",
    "        best_checkpoint = torch.load(os.path.join(checkpoint_dir, exp_name,\" source\",'model_best.pth.tar'))\n",
    "        print('Best model pack loaded')\n",
    "        loaded_epoch = best_checkpoint['iteration']\n",
    "        model.feature_extractor.load_state_dict(best_checkpoint['feature_extractor'])\n",
    "        model.classifier.load_state_dict(best_checkpoint['classifier'])\n",
    "        best_accuracy = best_checkpoint['test_acc']\n",
    "        optimizer.load_state_dict(best_checkpoint['optimizer'])\n",
    "        print(f\"current best test accuracy is: {best_accuracy}, at step: {loaded_epoch}\")\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, exp_name):\n",
    "    \"\"\"\n",
    "    save the checkpoint during training stage\n",
    "    :param state: content to be saved\n",
    "    :param is_best: if DPGN model's performance is the best at current step\n",
    "    :param exp_name: experiment name\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    torch.save(state, os.path.join('{}'.format(exp_name), 'checkpoint.pth.tar'))\n",
    "    if is_best:\n",
    "        shutil.copyfile(os.path.join('{}'.format(exp_name), 'checkpoint.pth.tar'),\n",
    "                        os.path.join('{}'.format(exp_name), 'model_best.pth.tar'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10, freeze_feature_extractor=False, dataset_part = \"source\"):\n",
    "    global best_accuracy\n",
    "    global best_target_accuracy\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    if freeze_feature_extractor:\n",
    "        for param in model.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        for param in model.feature_extractor.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    train_losses, val_losses, train_accuracies ,val_accuracies = [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "        train_acc = correct / total\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_acc = correct / total\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            print(f\"New Best Accuracy Found: {best_accuracy*100:.4f}, At epoch:{epoch + loaded_epoch + 1}. Saving.\")\n",
    "            save_checkpoint({\n",
    "                'iteration': epoch + loaded_epoch + 1,\n",
    "                'feature_extractor': model.feature_extractor.state_dict(),\n",
    "                'classifier': model.classifier.state_dict(),\n",
    "                'test_acc': best_accuracy,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }, True, os.path.join(checkpoint_dir, exp_name, dataset_part))\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies ,val_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T12:34:56.974736Z",
     "start_time": "2025-06-26T12:34:56.965294Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Custom \"sciency\" look via manual rcParams\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.edgecolor': 'black',\n",
    "    'axes.linewidth': 1.2,\n",
    "    'xtick.labelsize': 11,\n",
    "    'ytick.labelsize': 11,\n",
    "    'legend.fontsize': 11,\n",
    "    'grid.color': 'gray',\n",
    "    'grid.linestyle': '--',\n",
    "    'grid.alpha': 0.4,\n",
    "    'lines.linewidth': 2.5,\n",
    "})\n",
    "\n",
    "# Custom high-contrast color palette\n",
    "custom_colors = ['#009E73', '#0072B2']\n",
    "\n",
    "\n",
    "def plot_results(train_losses, val_losses, train_accuracies ,val_accuracies):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss', color=custom_colors[0])\n",
    "    plt.plot(val_losses, label='Validation Loss', color=custom_colors[1])\n",
    "    plt.title('Loss Over Epochs', weight='bold')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy', color=custom_colors[0])\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy', color=custom_colors[1])\n",
    "    plt.title('Accuracy Over Epochs', weight='bold')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"fig.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "def plot_confusion_matrix(model, data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    p = cm\n",
    "    f1_fault = p[1,1] / (p[1,1] + (0.5 * (p[0,1] + p[1,0]))) * 100\n",
    "    f1_health = p[0,0] / (p[0,0] + (0.5 * (p[0,1] + p[1,0]))) * 100\n",
    "    accuracy = (p[0,0] + p[1,1]) / (p[0,0] + p[1,1] + p[0,1] + p[1,0]) * 100\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Healthy F1 Score: {f1_health:.4f}, Faulty F1 Score: {f1_fault:.4f}\")\n",
    "    print(cm)\n",
    "    return accuracy, f1_fault"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train the model on the A1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_losses, val_losses, train_accuracies ,val_accuracies = train_model(model, train_loader, val_loader, epochs=100, freeze_feature_extractor=False, dataset_part = \"source\")\n",
    "plot_results(train_losses, val_losses, train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the best pretraining model with the highest performance on the source validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T12:34:41.502408Z",
     "start_time": "2025-06-24T12:34:41.361394Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Found a checkpoint, loading checkpoint from\")\n",
    "\n",
    "best_checkpoint = torch.load(\"model_best.pth.tar\")\n",
    "print('Best model pack loaded')\n",
    "loaded_epoch = best_checkpoint['iteration']\n",
    "model.feature_extractor.load_state_dict(best_checkpoint['feature_extractor'])\n",
    "model.classifier.load_state_dict(best_checkpoint['classifier'])\n",
    "best_accuracy = best_checkpoint['test_acc']\n",
    "optimizer.load_state_dict(best_checkpoint['optimizer'])\n",
    "print(f\"current best test accuracy is: {best_accuracy}, at step: {loaded_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T12:34:41.509656Z",
     "start_time": "2025-06-24T12:34:41.503632Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        # Not sure there are any global tensors in the state dict\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)\n",
    "optimizer_to(optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T12:34:41.516432Z",
     "start_time": "2025-06-24T12:34:41.510623Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pre_trained_model():\n",
    "    print(f\"Found a checkpoint, loading checkpoint from\")\n",
    "    # best_checkpoint = torch.load(os.path.join(checkpoint_dir, exp_name,\"source\",'model_best.pth.tar'))\n",
    "    best_checkpoint = torch.load(\"model_best.pth.tar\", weights_only=False)\n",
    "    print('Best model pack loaded')\n",
    "    loaded_epoch = best_checkpoint['iteration']\n",
    "    model.feature_extractor.load_state_dict(best_checkpoint['feature_extractor'])\n",
    "    model.classifier.load_state_dict(best_checkpoint['classifier'])\n",
    "    best_accuracy = best_checkpoint['test_acc']\n",
    "    optimizer.load_state_dict(best_checkpoint['optimizer'])\n",
    "    print(f\"current best test accuracy is: {best_accuracy}, at step: {loaded_epoch}\")\n",
    "\n",
    "    def optimizer_to(optim, device):\n",
    "        for param in optim.state.values():\n",
    "            # Not sure there are any global tensors in the state dict\n",
    "            if isinstance(param, torch.Tensor):\n",
    "                param.data = param.data.to(device)\n",
    "                if param._grad is not None:\n",
    "                    param._grad.data = param._grad.data.to(device)\n",
    "            elif isinstance(param, dict):\n",
    "                for subparam in param.values():\n",
    "                    if isinstance(subparam, torch.Tensor):\n",
    "                        subparam.data = subparam.data.to(device)\n",
    "                        if subparam._grad is not None:\n",
    "                            subparam._grad.data = subparam._grad.data.to(device)\n",
    "    optimizer_to(optimizer, device)\n",
    "    return model, optimizer\n",
    "\n",
    "def get_fine_tuned_model():\n",
    "    print(f\"Found a checkpoint, loading checkpoint from\")\n",
    "    best_checkpoint = torch.load(os.path.join(checkpoint_dir, exp_name,\"target\",'model_best.pth.tar'),weights_only=False)\n",
    "    print('Best model pack loaded')\n",
    "    loaded_epoch = best_checkpoint['iteration']\n",
    "    model.feature_extractor.load_state_dict(best_checkpoint['feature_extractor'])\n",
    "    model.classifier.load_state_dict(best_checkpoint['classifier'])\n",
    "    best_accuracy = best_checkpoint['test_acc']\n",
    "    optimizer.load_state_dict(best_checkpoint['optimizer'])\n",
    "    print(f\"current best test accuracy is: {best_accuracy}, at step: {loaded_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune the pre-trained model with 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T12:50:54.469959Z",
     "start_time": "2025-06-24T12:34:41.517207Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "combined_dataset = ConcatDataset([target_train_loader.dataset, target_val_loader.dataset])\n",
    "\n",
    "# Step 2: Setup KFold\n",
    "k_folds = 10\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "f1s_fault = []\n",
    "fine_tuned_models = []\n",
    "# Step 3: Start cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(combined_dataset)):\n",
    "    print(f\"=======================================================\")\n",
    "    print(f\"=======================================================\")\n",
    "    print(f\"Fold {fold+1}/{k_folds}\")\n",
    "\n",
    "    # Create data loaders for the current fold\n",
    "    train_subset = Subset(combined_dataset, train_idx)\n",
    "    val_subset = Subset(combined_dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model (you might want to reinitialize model weights each fold)\n",
    "    model, optimizer = get_pre_trained_model()\n",
    "\n",
    "    # Train model\n",
    "    best_accuracy = 0\n",
    "    loaded_epoch = 0\n",
    "\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        target_test_loader,\n",
    "        epochs=100,\n",
    "        freeze_feature_extractor=False,\n",
    "        dataset_part=\"target\"\n",
    "    )\n",
    "    print((train_losses, val_losses, train_accuracies, val_accuracies))\n",
    "\n",
    "    best_model = get_fine_tuned_model()\n",
    "    fine_tuned_models.append(model)\n",
    "    current_accuracy, current_f1_fault = plot_confusion_matrix(model, target_test_loader)\n",
    "    accuracies.append(current_accuracy)\n",
    "    f1s_fault.append(current_f1_fault)\n",
    "\n",
    "print(f\"{np.array(accuracies).mean()} +- {np.array(accuracies).std()}\")\n",
    "print(f\"{np.array(f1s_fault).mean()} +- {np.array(f1s_fault).std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T13:26:35.293857Z",
     "start_time": "2025-06-24T13:26:34.862891Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "for i, model in enumerate(fine_tuned_models):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in target_test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)  # (batch_size, 2)\n",
    "            pos_probs = probs[:, 1]  # Class 1\n",
    "\n",
    "            all_probs.extend(pos_probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(roc_auc)\n",
    "\n",
    "    ax.plot(fpr, tpr, lw=1, alpha=0.3, label=f\"ROC Model {i} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "# Mean + std deviation across models\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "\n",
    "ax.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"b\",\n",
    "    label=r\"Mean ROC (AUC = %0.2f Â± %0.2f)\" % (mean_auc, std_auc),\n",
    "    lw=2,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=1, color=\"gray\", alpha=0.8)\n",
    "\n",
    "ax.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"ROC Curves from Fine-Tuned Models\",\n",
    ")\n",
    "plt.title(\"ROC Curves from Fine-Tuned Models\", weight='bold')\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T12:50:54.984653Z",
     "start_time": "2025-06-24T12:50:54.981349Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "\n",
    "def convert_loader_to_background_dict(dataloader, max_samples=1000):\n",
    "    \"\"\"\n",
    "    Converts a PyTorch DataLoader to a background data dictionary for deep integration.\n",
    "\n",
    "    Parameters:\n",
    "        dataloader (DataLoader): PyTorch DataLoader with each sample shaped (m x n).\n",
    "        max_samples (int): Maximum number of samples to use for background data.\n",
    "\n",
    "    Returns:\n",
    "        dict: {'pytorch': [torch.Tensor]} where the tensor is (N, m, n)\n",
    "    \"\"\"\n",
    "    background_samples = []\n",
    "    total_collected = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # Unpack batch: assume (data, label)\n",
    "        if isinstance(batch, (list, tuple)):\n",
    "            data = batch[0]\n",
    "        else:\n",
    "            data = batch\n",
    "\n",
    "        for sample in data:\n",
    "            background_samples.append(sample)\n",
    "            total_collected += 1\n",
    "            if total_collected >= max_samples:\n",
    "                break\n",
    "        if total_collected >= max_samples:\n",
    "            break\n",
    "\n",
    "    # Stack into a single tensor (N, m, n)\n",
    "    background_tensor = torch.stack(background_samples)\n",
    "    return background_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T12:50:55.008973Z",
     "start_time": "2025-06-24T12:50:54.985529Z"
    }
   },
   "outputs": [],
   "source": [
    "test_tensor = convert_loader_to_background_dict(target_test_loader).to(device)\n",
    "train_tensor = convert_loader_to_background_dict(target_train_loader).to(device)\n",
    "\n",
    "explainer = shap.DeepExplainer(model, train_tensor)\n",
    "shap_values = explainer.shap_values(test_tensor, check_additivity = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T12:51:01.212356Z",
     "start_time": "2025-06-24T12:51:01.209705Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = [\"FIT101\", \"LIT101\", \"MV101\", \"P101\", \"AIT201\", \"AIT202\", \"AIT203\", \"FIT201\", \"MV201\", \"P203\", \"DPIT301\", \"FIT301\", \"LIT301\", \"MV301\", \"MV302\", \"MV303\", \"MV304\", \"AIT402\", \"FIT401\", \"LIT401\", \"AIT501\", \"AIT502\", \"AIT503\", \"AIT504\", \"FIT501\", \"FIT502\", \"FIT503\", \"PIT501\", \"PIT502\", \"PIT503\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T12:51:01.226870Z",
     "start_time": "2025-06-24T12:51:01.213206Z"
    }
   },
   "outputs": [],
   "source": [
    "class_0 = shap_values[0]\n",
    "class_0 = class_0.mean(axis = 1)\n",
    "df_0 = pd.DataFrame(class_0, columns=columns)\n",
    "\n",
    "value_tensor = test_tensor.clone()\n",
    "value_tensor = value_tensor.mean(axis = 1).to(\"cpu\")\n",
    "color_df = pd.DataFrame(value_tensor, columns=columns)\n",
    "color_df=(color_df-color_df.min())/(color_df.max() - color_df.min())\n",
    "\n",
    "\n",
    "df_long_0 = df_0.melt(var_name=\"Feature\", value_name=\"Value\")\n",
    "df_long_color = color_df.melt(var_name=\"Feature\", value_name=\"Color\")\n",
    "\n",
    "df_long_0[\"Color\"] = df_long_color[\"Color\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T12:51:10.842302Z",
     "start_time": "2025-06-24T12:51:01.227762Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn\n",
    "sns.set_theme(rc={'figure.figsize':(16,8)})\n",
    "seaborn.swarmplot(data=df_long_0, x=\"Feature\", y=\"Value\", size=2, hue=\"Color\", )\n",
    "# seaborn.violinplot(data=df_long, x=\"Feature\", y=\"Value\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T12:51:10.847758Z",
     "start_time": "2025-06-24T12:51:10.843346Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_importance = np.mean(np.abs(class_0), axis=0)\n",
    "\n",
    "# Create a DataFrame for easy viewing\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': columns,   # list of feature names\n",
    "    'Importance': feature_importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "sorted_features = importance_df.sort_values(by=\"Importance\", ascending=False)[\"Feature\"].tolist()\n",
    "df_long_0[\"Feature\"] = pd.Categorical(df_long_0[\"Feature\"], categories=sorted_features, ordered=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T12:51:14.916023Z",
     "start_time": "2025-06-24T12:51:10.848669Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "mpl.style.use('default')\n",
    "plt.rcParams['grid.color'] = (0.5, 0.5, 0.5, 0.1)\n",
    "\n",
    "custom_colors = ['#0072B2', '#700882',  '#e83517']\n",
    "cm = LinearSegmentedColormap.from_list(\n",
    "        \"Custom\", custom_colors, N=20)\n",
    "\n",
    "\n",
    "\n",
    "df_long_0[\"Feature\"] = pd.Categorical(df_long_0[\"Feature\"], categories=sorted_features, ordered=True)\n",
    "important_feature_count = 15\n",
    "important_feature = sorted_features[:important_feature_count]\n",
    "df_long_important_features =  df_long_0[df_long_0[\"Feature\"].isin(important_feature)]\n",
    "df_long_important_features[\"Feature\"] = df_long_important_features[\"Feature\"].cat.remove_unused_categories()\n",
    "# 3. Plot with seaborn using the new sorted Feature order\n",
    "plt.figure(figsize=(11, 7))\n",
    "sns.swarmplot(data=df_long_important_features, x=\"Feature\", y=\"Value\", size=5, hue=\"Color\", palette=cm, legend = False)\n",
    "\n",
    "plt.title('SHAP values: impact on model output', weight='bold')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.axhline(y=0, color='black', linestyle='-',lw = 0.5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T12:51:15.172266Z",
     "start_time": "2025-06-24T12:51:14.919313Z"
    }
   },
   "outputs": [],
   "source": [
    "importance_df[\"Importance\"] = importance_df[\"Importance\"] / importance_df[\"Importance\"].sum()\n",
    "# plt.xticks(rotation=90)\n",
    "plt.figure(figsize=(8, 10))\n",
    "seaborn.barplot(data = importance_df, x = \"Importance\", y = \"Feature\", orient = 'h', color = custom_colors[0])\n",
    "plt.grid(True)\n",
    "plt.title(\"SHAP Feature Importance\", weight='bold')\n",
    "plt.ylabel(\"Features\")\n",
    "plt.xlabel(\"SHAP Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6646861,
     "sourceId": 10722520,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6741114,
     "sourceId": 10853383,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
